{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb3b58ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training RL Agent (X) vs Random Player (O) ---\n",
      "\n",
      "--- Training RL Agent (X) vs RL Agent (O) (Self-Play) ---\n",
      "Starting training for 70000 episodes...\n",
      "Episode 7000/70000 completed. Agent1 Wins: 4144, Losses: 2004, Draws: 852\n",
      "  Agent1 Epsilon: 0.9324\n",
      "  Agent2 Epsilon: 0.9324\n",
      "Episode 14000/70000 completed. Agent1 Wins: 8283, Losses: 4042, Draws: 1675\n",
      "  Agent1 Epsilon: 0.8694\n",
      "  Agent2 Epsilon: 0.8694\n",
      "Episode 21000/70000 completed. Agent1 Wins: 12327, Losses: 6126, Draws: 2547\n",
      "  Agent1 Epsilon: 0.8106\n",
      "  Agent2 Epsilon: 0.8106\n",
      "Episode 28000/70000 completed. Agent1 Wins: 16360, Losses: 8179, Draws: 3461\n",
      "  Agent1 Epsilon: 0.7558\n",
      "  Agent2 Epsilon: 0.7558\n",
      "Episode 35000/70000 completed. Agent1 Wins: 20368, Losses: 10179, Draws: 4453\n",
      "  Agent1 Epsilon: 0.7047\n",
      "  Agent2 Epsilon: 0.7047\n",
      "Episode 42000/70000 completed. Agent1 Wins: 24356, Losses: 12192, Draws: 5452\n",
      "  Agent1 Epsilon: 0.6570\n",
      "  Agent2 Epsilon: 0.6570\n",
      "Episode 49000/70000 completed. Agent1 Wins: 28224, Losses: 14191, Draws: 6585\n",
      "  Agent1 Epsilon: 0.6126\n",
      "  Agent2 Epsilon: 0.6126\n",
      "Episode 56000/70000 completed. Agent1 Wins: 32163, Losses: 16066, Draws: 7771\n",
      "  Agent1 Epsilon: 0.5712\n",
      "  Agent2 Epsilon: 0.5712\n",
      "Episode 63000/70000 completed. Agent1 Wins: 36031, Losses: 17866, Draws: 9103\n",
      "  Agent1 Epsilon: 0.5326\n",
      "  Agent2 Epsilon: 0.5326\n",
      "Episode 70000/70000 completed. Agent1 Wins: 39777, Losses: 19782, Draws: 10441\n",
      "  Agent1 Epsilon: 0.4966\n",
      "  Agent2 Epsilon: 0.4966\n",
      "\n",
      "Training finished.\n",
      "Agent1 ('X') Wins: 39777 (56.8%)\n",
      "Opponent ('O') Wins: 19782 (28.3%)\n",
      "Draws: 10441 (14.9%)\n",
      "Final Agent1 Epsilon: 0.4966\n",
      "Final Agent2 Epsilon: 0.4966\n",
      "Agent1 Q-table size: 2423\n",
      "Agent2 Q-table size: 2097\n",
      "Q-table saved to agent_X_selfplay.pkl\n",
      "Q-table saved to agent_O_selfplay.pkl\n",
      "Q-table loaded from agent_X_selfplay.pkl\n",
      "\n",
      "--- Evaluating Trained RL Agent (X) vs Random Player (O) ---\n",
      "\n",
      "Evaluating Agent 'X' vs Agent 'O' for 1000 games...\n",
      "\n",
      "Evaluation Results:\n",
      "Agent 'X' Wins: 984 (98.4%)\n",
      "Agent 'O' Wins: 0 (0.0%)\n",
      "Draws: 16 (1.6%)\n",
      "\n",
      "--- Evaluating Trained RL Agent (X) vs Trained RL Agent (O) ---\n",
      "\n",
      "Evaluating Agent 'X' vs Agent 'O' for 1000 games...\n",
      "\n",
      "Evaluation Results:\n",
      "Agent 'X' Wins: 0 (0.0%)\n",
      "Agent 'O' Wins: 0 (0.0%)\n",
      "Draws: 1000 (100.0%)\n",
      "\n",
      "--- Evaluating Trained RL Agent (X) vs Untrained RL Agent (O) ---\n",
      "\n",
      "Evaluating Agent 'X' vs Agent 'O' for 1000 games...\n",
      "\n",
      "Evaluation Results:\n",
      "Agent 'X' Wins: 981 (98.1%)\n",
      "Agent 'O' Wins: 0 (0.0%)\n",
      "Draws: 19 (1.9%)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "\n",
    "# --- I. Tic-Tac-Toe Environment Implementation ---\n",
    "class TicTacToe:\n",
    "    def __init__(self):\n",
    "        self.board = np.full((3, 3), ' ')\n",
    "        self.winning_combinations = [\n",
    "            # Rows\n",
    "            [(0, 0), (0, 1), (0, 2)], [(1, 0), (1, 1), (1, 2)], [(2, 0), (2, 1), (2, 2)],\n",
    "            # Columns\n",
    "            [(0, 0), (1, 0), (2, 0)], [(0, 1), (1, 1), (2, 1)], [(0, 2), (1, 2), (2, 2)],\n",
    "            # Diagonals\n",
    "            [(0, 0), (1, 1), (2, 2)], [(0, 2), (1, 1), (2, 0)]\n",
    "        ]\n",
    "        self.termination_state = np.full((3, 3), 'T')\n",
    "\n",
    "    def reset_board(self):\n",
    "        self.board = np.full((3, 3), ' ')\n",
    "        return self.get_board_hash()\n",
    "\n",
    "    def get_board_hash(self, board=None):\n",
    "        \"\"\"Returns a hashable representation of the board state.\"\"\"\n",
    "        current_board = board if board is not None else self.board\n",
    "        return tuple(current_board.flatten())\n",
    "\n",
    "    def get_available_moves(self):\n",
    "        \"\"\"Returns a list of (row, col) tuples for empty cells.\"\"\"\n",
    "        moves = []\n",
    "        for r in range(3):\n",
    "            for c in range(3):\n",
    "                if self.board[r, c] == ' ':\n",
    "                    moves.append((r, c))\n",
    "        return moves\n",
    "\n",
    "    def make_move(self, move, player_symbol):\n",
    "        \"\"\"Makes a move on the board. move is a (row, col) tuple.\"\"\"\n",
    "        if self.board[move[0], move[1]] == ' ':\n",
    "            self.board[move[0], move[1]] = player_symbol\n",
    "            return True\n",
    "        return False # Invalid move\n",
    "\n",
    "    def check_winner(self):\n",
    "        \"\"\"\n",
    "        Checks for a winner or a draw.\n",
    "        Returns:\n",
    "            'X' if X wins\n",
    "            'O' if O wins\n",
    "            'draw' if it's a draw\n",
    "            None if the game is ongoing\n",
    "        \"\"\"\n",
    "        for player_symbol in ['X', 'O']:\n",
    "            for combo in self.winning_combinations:\n",
    "                if all(self.board[r, c] == player_symbol for r, c in combo):\n",
    "                    return player_symbol\n",
    "        if not self.get_available_moves(): # No more moves\n",
    "            return 'draw'\n",
    "        return None # Game is ongoing\n",
    "\n",
    "    def get_reward(self, player_symbol, winner):\n",
    "        \"\"\"\n",
    "        Calculates reward for the player.\n",
    "        Args:\n",
    "            player_symbol (str): The symbol of the agent ('X' or 'O').\n",
    "            winner (str/None): Result from check_winner().\n",
    "        Returns:\n",
    "            int: Reward value.\n",
    "        \"\"\"\n",
    "        if winner == player_symbol:\n",
    "            return 1  # Win\n",
    "        elif winner is not None and winner != 'draw': # Opponent won\n",
    "            return -1 # Loss\n",
    "        # elif winner == 'draw':\n",
    "        #     return 0 # Draw\n",
    "        else:\n",
    "            return 0 \n",
    "\n",
    "\n",
    "    def is_game_over(self):\n",
    "        return self.check_winner() is not None\n",
    "\n",
    "    def render_board(self):\n",
    "        print(\"-------------\")\n",
    "        for row in self.board:\n",
    "            print(f\"| {' | '.join(row)} |\")\n",
    "            print(\"-------------\")\n",
    "        print()\n",
    "\n",
    "# --- II. Reinforcement Learning Agent ---\n",
    "class RLAgent:\n",
    "    def __init__(self, player_symbol, learning_rate=0.1, discount_factor=0.9, exploration_rate=1.0, exploration_decay_rate=0.999, min_exploration_rate=0.01, lr_decay=0.9995):\n",
    "        self.player_symbol = player_symbol\n",
    "        self.opponent_symbol = 'O' if player_symbol == 'X' else 'X'\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = discount_factor\n",
    "        self.epsilon = exploration_rate\n",
    "        self.epsilon_decay = exploration_decay_rate\n",
    "        self.min_epsilon = min_exploration_rate\n",
    "        self.q_table = defaultdict(lambda: defaultdict(float)) # Q(s,a) -> value. state is board hash, action is (r,c)\n",
    "        self.lr_decay = lr_decay\n",
    "\n",
    "    def get_action(self, board_hash, available_moves):\n",
    "        \"\"\"Epsilon-greedy action selection.\"\"\"\n",
    "        if not available_moves:\n",
    "            return None\n",
    "\n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            return random.choice(available_moves)  # Explore\n",
    "        else:\n",
    "            # Exploit: Choose the action with the highest Q-value\n",
    "            q_values_for_state = self.q_table[board_hash]\n",
    "            if not q_values_for_state: # No Q-values for this state yet, pick randomly\n",
    "                return random.choice(available_moves)\n",
    "\n",
    "            max_q = -float('inf')\n",
    "            best_action = None\n",
    "            # Shuffle available moves to break ties randomly if Q-values are the same\n",
    "            random.shuffle(available_moves)\n",
    "            for move in available_moves:\n",
    "                if q_values_for_state[move] > max_q:\n",
    "                    max_q = q_values_for_state[move]\n",
    "                    best_action = move\n",
    "                elif q_values_for_state[move] == -float('inf') and best_action is None: # if all are -inf, pick one\n",
    "                     best_action = move\n",
    "\n",
    "\n",
    "            if best_action is None: # If all Q-values are 0 or not set, pick randomly\n",
    "                return random.choice(available_moves)\n",
    "            return best_action\n",
    "\n",
    "    def update_q_table(self, state_hash, action, reward, next_state_hash, next_available_moves, is_terminal):\n",
    "        \"\"\"\n",
    "        Update Q-value for a state-action pair using Q-learning.\n",
    "        This is the core of your training algorithm.\n",
    "        \"\"\"\n",
    "        if action is None: # Should not happen if get_action is called with available_moves\n",
    "            return\n",
    "\n",
    "        current_q = self.q_table[state_hash][action]\n",
    "\n",
    "        # Q-learning: Find max Q-value for the next state\n",
    "        max_next_q = 0.0\n",
    "        if not is_terminal and next_available_moves:\n",
    "            q_values_for_next_state = self.q_table[next_state_hash]\n",
    "            if q_values_for_next_state: # if there are entries for next_state\n",
    "                 max_next_q = max(q_values_for_next_state[move] for move in next_available_moves if move in q_values_for_next_state) if q_values_for_next_state else 0.0\n",
    "            # If next_state_hash is not in q_table or no moves have q_values yet for it, max_next_q remains 0\n",
    "\n",
    "        # Q-learning update rule\n",
    "        new_q = current_q + self.lr * (reward + self.gamma * max_next_q - current_q)\n",
    "        self.q_table[state_hash][action] = new_q\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.min_epsilon, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "    def decay_learning_rate(self):\n",
    "        self.lr = self.lr * self.lr_decay\n",
    "\n",
    "    def save_q_table(self, filename=\"q_table.pkl\"):\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(dict(self.q_table), f) # Convert defaultdict to dict for pickling\n",
    "        print(f\"Q-table saved to {filename}\")\n",
    "\n",
    "    def load_q_table(self, filename=\"q_table.pkl\"):\n",
    "        try:\n",
    "            with open(filename, 'rb') as f:\n",
    "                loaded_q_table = pickle.load(f)\n",
    "                self.q_table = defaultdict(lambda: defaultdict(float), loaded_q_table)\n",
    "                # Convert inner dicts back to defaultdict(float) if needed,\n",
    "                # but direct assignment usually works if structure is q_table[state][action]\n",
    "                for state_key in self.q_table:\n",
    "                    self.q_table[state_key] = defaultdict(float, self.q_table[state_key])\n",
    "\n",
    "            print(f\"Q-table loaded from {filename}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"No Q-table found at {filename}, starting with an empty one.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading Q-table: {e}. Starting with an empty one.\")\n",
    "\n",
    "\n",
    "# --- III. Training the Agent ---\n",
    "def train_agent(agent1, agent2_opponent, env, num_episodes=10000):\n",
    "    print(f\"Starting training for {num_episodes} episodes...\")\n",
    "    wins_agent1 = 0\n",
    "    draws = 0\n",
    "    losses_agent1 = 0 # Agent1 losses / Agent2 wins\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        current_state_hash = env.reset_board()\n",
    "        game_over = False\n",
    "        last_move_info = {agent1.player_symbol: None, agent2_opponent.player_symbol: None} # To store (s, a) for update\n",
    "\n",
    "        current_player = agent1 # Agent1 starts\n",
    "\n",
    "        while not game_over:\n",
    "            available_moves = env.get_available_moves()\n",
    "            if not available_moves: # Should be caught by game_over, but as a safeguard\n",
    "                break\n",
    "\n",
    "            is_agent1_turn = current_player == agent1\n",
    "\n",
    "            if is_agent1_turn:\n",
    "                action = agent1.get_action(current_state_hash, available_moves)\n",
    "                if action:\n",
    "                    # Store state and action for agent1 before making the move\n",
    "                    last_move_info[agent1.player_symbol] = {'state': current_state_hash, 'action': action}\n",
    "                    env.make_move(action, agent1.player_symbol)\n",
    "                else: # No valid action (should not happen with available_moves check)\n",
    "                    game_over = True # End game if agent can't move\n",
    "                    break\n",
    "            else: # Opponent's turn (could be another RLAgent or a random one)\n",
    "                if isinstance(agent2_opponent, RLAgent):\n",
    "                    action = agent2_opponent.get_action(current_state_hash, available_moves)\n",
    "                    if action:\n",
    "                        # Store state and action for agent2 if it's an RL agent\n",
    "                        last_move_info[agent2_opponent.player_symbol] = {'state': current_state_hash, 'action': action}\n",
    "                        env.make_move(action, agent2_opponent.player_symbol)\n",
    "                    else:\n",
    "                        game_over = True\n",
    "                        break\n",
    "                else: # Random opponent\n",
    "                    action = random.choice(available_moves)\n",
    "                    env.make_move(action, agent2_opponent.player_symbol) # Opponent symbol\n",
    "\n",
    "            winner = env.check_winner()\n",
    "            game_over = (winner is not None)\n",
    "            next_state_hash = env.get_board_hash()\n",
    "            next_available_moves = env.get_available_moves()\n",
    "\n",
    "            if is_agent1_turn:\n",
    "                if isinstance(agent2_opponent, RLAgent) and last_move_info[agent2_opponent.player_symbol]:\n",
    "                    s, a = last_move_info[agent2_opponent.player_symbol]['state'], last_move_info[agent2_opponent.player_symbol]['action']\n",
    "                    reward_agent2 = env.get_reward(agent2_opponent.player_symbol, winner)\n",
    "                    # Note: The reward for agent2 is based on the same game outcome\n",
    "                    agent2_opponent.update_q_table(s, a, reward_agent2, next_state_hash, next_available_moves, game_over)\n",
    "                    last_move_info[agent2_opponent.player_symbol] = None # Clear after update\n",
    "\n",
    "            else:\n",
    "                if last_move_info[agent1.player_symbol]:\n",
    "                    s, a = last_move_info[agent1.player_symbol]['state'], last_move_info[agent1.player_symbol]['action']\n",
    "                    reward_agent1 = env.get_reward(agent1.player_symbol, winner)\n",
    "                    agent1.update_q_table(s, a, reward_agent1, next_state_hash, next_available_moves, game_over)\n",
    "                    last_move_info[agent1.player_symbol] = None\n",
    "\n",
    "            # --- THIS IS WHERE THE TRAINING ALGORITHM IS APPLIED ---\n",
    "            ### But what if the game is over? We need to update the Q-table for the last move but it's not what done in here\n",
    "            # Update Q-table for agent1 based on its last move\n",
    "            # if last_move_info[agent1.player_symbol]:\n",
    "            #     s, a = last_move_info[agent1.player_symbol]['state'], last_move_info[agent1.player_symbol]['action']\n",
    "            #     reward_agent1 = env.get_reward(agent1.player_symbol, winner)\n",
    "            #     agent1.update_q_table(s, a, reward_agent1, next_state_hash, next_available_moves, game_over)\n",
    "            #     last_move_info[agent1.player_symbol] = None # Clear after update\n",
    "\n",
    "            # # Update Q-table for agent2 if it's an RLAgent and made a move\n",
    "            # if isinstance(agent2_opponent, RLAgent) and last_move_info[agent2_opponent.player_symbol]:\n",
    "            #     s, a = last_move_info[agent2_opponent.player_symbol]['state'], last_move_info[agent2_opponent.player_symbol]['action']\n",
    "            #     reward_agent2 = env.get_reward(agent2_opponent.player_symbol, winner)\n",
    "            #     # Note: The reward for agent2 is based on the same game outcome\n",
    "            #     agent2_opponent.update_q_table(s, a, reward_agent2, next_state_hash, next_available_moves, game_over)\n",
    "            #     last_move_info[agent2_opponent.player_symbol] = None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            current_state_hash = next_state_hash\n",
    "\n",
    "            if game_over:\n",
    "                #update for the last move\n",
    "                # Update Q-table for agent1 based on its last move\n",
    "                termination_state_hash = env.termination_state\n",
    "                if last_move_info[agent1.player_symbol]:\n",
    "                    s, a = last_move_info[agent1.player_symbol]['state'], last_move_info[agent1.player_symbol]['action']\n",
    "                    reward_agent1 = env.get_reward(agent1.player_symbol, winner)\n",
    "                    agent1.update_q_table(s, a, reward_agent1, termination_state_hash, next_available_moves, game_over)\n",
    "                    last_move_info[agent1.player_symbol] = None # Clear after update\n",
    "\n",
    "                # Update Q-table for agent2 if it's an RLAgent and made a move\n",
    "                if isinstance(agent2_opponent, RLAgent) and last_move_info[agent2_opponent.player_symbol]:\n",
    "                    s, a = last_move_info[agent2_opponent.player_symbol]['state'], last_move_info[agent2_opponent.player_symbol]['action']\n",
    "                    reward_agent2 = env.get_reward(agent2_opponent.player_symbol, winner)\n",
    "                    # Note: The reward for agent2 is based on the same game outcome\n",
    "                    agent2_opponent.update_q_table(s, a, reward_agent2, termination_state_hash, next_available_moves, game_over)\n",
    "                    last_move_info[agent2_opponent.player_symbol] = None\n",
    "\n",
    "                if winner == agent1.player_symbol:\n",
    "                    wins_agent1 += 1\n",
    "                elif winner == agent2_opponent.player_symbol:\n",
    "                    losses_agent1 +=1\n",
    "                elif winner == 'draw':\n",
    "                    draws += 1\n",
    "            else:\n",
    "                # Switch player\n",
    "                current_player = agent2_opponent if is_agent1_turn else agent1\n",
    "\n",
    "\n",
    "        if isinstance(agent1, RLAgent): agent1.decay_epsilon()\n",
    "        if isinstance(agent2_opponent, RLAgent): agent2_opponent.decay_epsilon()\n",
    "        if isinstance(agent1, RLAgent): agent1.decay_learning_rate()\n",
    "        if isinstance(agent2_opponent, RLAgent): agent2_opponent.decay_learning_rate()\n",
    "\n",
    "        if (episode + 1) % (num_episodes // 10) == 0:\n",
    "            print(f\"Episode {episode + 1}/{num_episodes} completed. Agent1 Wins: {wins_agent1}, Losses: {losses_agent1}, Draws: {draws}\")\n",
    "            if isinstance(agent1, RLAgent) : print(f\"  Agent1 Epsilon: {agent1.epsilon:.4f}\")\n",
    "            if isinstance(agent2_opponent, RLAgent) : print(f\"  Agent2 Epsilon: {agent2_opponent.epsilon:.4f}\")\n",
    "\n",
    "\n",
    "    print(\"\\nTraining finished.\")\n",
    "    print(f\"Agent1 ('{agent1.player_symbol}') Wins: {wins_agent1} ({(wins_agent1/num_episodes)*100:.1f}%)\")\n",
    "    print(f\"Opponent ('{agent2_opponent.player_symbol}') Wins: {losses_agent1} ({(losses_agent1/num_episodes)*100:.1f}%)\")\n",
    "    print(f\"Draws: {draws} ({(draws/num_episodes)*100:.1f}%)\")\n",
    "    if isinstance(agent1, RLAgent): print(f\"Final Agent1 Epsilon: {agent1.epsilon:.4f}\")\n",
    "    if isinstance(agent2_opponent, RLAgent): print(f\"Final Agent2 Epsilon: {agent2_opponent.epsilon:.4f}\")\n",
    "    if isinstance(agent1, RLAgent): print(f\"Agent1 Q-table size: {len(agent1.q_table)}\")\n",
    "    if isinstance(agent2_opponent, RLAgent): print(f\"Agent2 Q-table size: {len(agent2_opponent.q_table)}\")\n",
    "\n",
    "\n",
    "# Simple Random Player for training or comparison\n",
    "class RandomPlayer:\n",
    "    def __init__(self, player_symbol):\n",
    "        self.player_symbol = player_symbol\n",
    "\n",
    "    def get_action(self, board_hash, available_moves): # board_hash is not used but kept for consistency\n",
    "        if not available_moves:\n",
    "            return None\n",
    "        return random.choice(available_moves)\n",
    "\n",
    "# --- IV. Qualitative Performance Check: Play Against the Trained Agent ---\n",
    "def play_against_agent(agent, env):\n",
    "    print(\"\\nStarting game: Human (O) vs Trained Agent (X)\")\n",
    "    agent.epsilon = 0 # Ensure agent plays greedily\n",
    "    human_player = 'O'\n",
    "    current_player_symbol = 'X' # Agent starts or choose randomly\n",
    "\n",
    "    env.reset_board()\n",
    "    env.render_board()\n",
    "\n",
    "    while not env.is_game_over():\n",
    "        available_moves = env.get_available_moves()\n",
    "        if not available_moves:\n",
    "            break\n",
    "\n",
    "        if current_player_symbol == agent.player_symbol:\n",
    "            print(f\"Agent '{agent.player_symbol}' is thinking...\")\n",
    "            move = agent.get_action(env.get_board_hash(), available_moves)\n",
    "            if move is None: # Should not happen if there are available moves\n",
    "                print(\"Agent could not find a move.\")\n",
    "                break\n",
    "            print(f\"Agent '{agent.player_symbol}' plays at {move}\")\n",
    "        else: # Human's turn\n",
    "            valid_move = False\n",
    "            while not valid_move:\n",
    "                try:\n",
    "                    print(f\"Available moves: {available_moves}\")\n",
    "                    row, col = map(int, input(f\"Your turn ({human_player}). Enter row,col (0-2): \").split(','))\n",
    "                    move = (row, col)\n",
    "                    if move in available_moves:\n",
    "                        valid_move = True\n",
    "                    else:\n",
    "                        print(\"Invalid move. Cell occupied or out of bounds.\")\n",
    "                except ValueError:\n",
    "                    print(\"Invalid input. Please enter row,col (e.g., 1,1).\")\n",
    "                except Exception as e:\n",
    "                    print(f\"An error occurred: {e}\")\n",
    "\n",
    "        env.make_move(move, current_player_symbol)\n",
    "        env.render_board()\n",
    "        winner = env.check_winner()\n",
    "\n",
    "        if winner:\n",
    "            if winner == 'draw':\n",
    "                print(\"It's a DRAW!\")\n",
    "            else:\n",
    "                print(f\"Player '{winner}' WINS!\")\n",
    "            break\n",
    "\n",
    "        current_player_symbol = human_player if current_player_symbol == agent.player_symbol else agent.player_symbol\n",
    "\n",
    "    if not env.is_game_over(): # If loop broke for other reasons\n",
    "        print(\"Game ended unexpectedly.\")\n",
    "\n",
    "# --- V. Quantitative Performance Measure: Two Agents Play Each Other ---\n",
    "def evaluate_agents(agent1, agent2, env, num_games=1000):\n",
    "    print(f\"\\nEvaluating Agent '{agent1.player_symbol}' vs Agent '{agent2.player_symbol}' for {num_games} games...\")\n",
    "    # Set agents to evaluation mode (no exploration)\n",
    "    original_eps1 = getattr(agent1, 'epsilon', None)\n",
    "    original_eps2 = getattr(agent2, 'epsilon', None)\n",
    "    if isinstance(agent1, RLAgent): agent1.epsilon = 0\n",
    "    if isinstance(agent2, RLAgent): agent2.epsilon = 0\n",
    "\n",
    "\n",
    "    scores = {agent1.player_symbol: 0, agent2.player_symbol: 0, 'draw': 0}\n",
    "    \n",
    "    # Alternate who starts\n",
    "    starts_player1 = True\n",
    "\n",
    "    for game_num in range(num_games):\n",
    "        env.reset_board()\n",
    "        game_over = False\n",
    "        \n",
    "        if starts_player1:\n",
    "            current_player = agent1\n",
    "            other_player = agent2\n",
    "        else:\n",
    "            current_player = agent2\n",
    "            other_player = agent1\n",
    "            \n",
    "        while not game_over:\n",
    "            available_moves = env.get_available_moves()\n",
    "            if not available_moves:\n",
    "                break # Should be handled by check_winner draw condition\n",
    "\n",
    "            board_hash = env.get_board_hash()\n",
    "            if current_player == agent1:\n",
    "                move = agent1.get_action(board_hash, available_moves)\n",
    "            else: # current_player == agent2\n",
    "                move = agent2.get_action(board_hash, available_moves)\n",
    "\n",
    "            if move:\n",
    "                env.make_move(move, current_player.player_symbol)\n",
    "            else: # Agent can't move, should not happen if game is not over and moves are available\n",
    "                print(f\"Warning: Agent {current_player.player_symbol} could not make a move in evaluation game {game_num+1}.\")\n",
    "                game_over = True # End game if an agent fails to move\n",
    "                scores['draw'] +=1 # Or count as a loss for the agent? For now, draw.\n",
    "                break\n",
    "\n",
    "\n",
    "            winner = env.check_winner()\n",
    "            if winner:\n",
    "                if winner == agent1.player_symbol:\n",
    "                    scores[agent1.player_symbol] += 1\n",
    "                elif winner == agent2.player_symbol:\n",
    "                    scores[agent2.player_symbol] += 1\n",
    "                else: # draw\n",
    "                    scores['draw'] += 1\n",
    "                game_over = True\n",
    "            \n",
    "            # Switch players\n",
    "            current_player, other_player = other_player, current_player\n",
    "            \n",
    "        #starts_player1 = not starts_player1 # Alternate starting player for next game\n",
    "\n",
    "    # Restore epsilon if they were RLAgents\n",
    "    if isinstance(agent1, RLAgent) and original_eps1 is not None: agent1.epsilon = original_eps1\n",
    "    if isinstance(agent2, RLAgent) and original_eps2 is not None: agent2.epsilon = original_eps2\n",
    "\n",
    "    print(\"\\nEvaluation Results:\")\n",
    "    print(f\"Agent '{agent1.player_symbol}' Wins: {scores[agent1.player_symbol]} ({(scores[agent1.player_symbol]/num_games)*100:.1f}%)\")\n",
    "    print(f\"Agent '{agent2.player_symbol}' Wins: {scores[agent2.player_symbol]} ({(scores[agent2.player_symbol]/num_games)*100:.1f}%)\")\n",
    "    print(f\"Draws: {scores['draw']} ({(scores['draw']/num_games)*100:.1f}%)\")\n",
    "    return scores\n",
    "\n",
    "\n",
    "# --- Example Usage ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize Environment\n",
    "    game_env = TicTacToe()\n",
    "\n",
    "    # Initialize Agents\n",
    "    # Agent 1 will be an RL Agent ('X')\n",
    "    rl_agent_X = RLAgent(player_symbol='X', learning_rate=0.1, discount_factor=1.0, exploration_rate=1.0, exploration_decay_rate=0.99999, min_exploration_rate=0.01, lr_decay=1)\n",
    "\n",
    "    # Agent 2 can be another RL Agent or a RandomPlayer for training\n",
    "    # Option 1: Train against a Random Player\n",
    "    random_opponent_O = RandomPlayer(player_symbol='O')\n",
    "    print(\"--- Training RL Agent (X) vs Random Player (O) ---\")\n",
    "    # train_agent(rl_agent_X, random_opponent_O, game_env, num_episodes=50000) # More episodes for better learning\n",
    "    # rl_agent_X.save_q_table(\"agent_X_vs_random.pkl\")\n",
    "\n",
    "    # Option 2: Train two RL agents against each other (self-play)\n",
    "    rl_agent_O = RLAgent(player_symbol='O', learning_rate=0.1, discount_factor=1.0, exploration_rate=1.0, exploration_decay_rate=0.99999, min_exploration_rate=0.01, lr_decay=1)\n",
    "    print(\"\\n--- Training RL Agent (X) vs RL Agent (O) (Self-Play) ---\")\n",
    "    train_agent(rl_agent_X, rl_agent_O, game_env, num_episodes=70000) # Self-play often requires more episodes\n",
    "    rl_agent_X.save_q_table(\"agent_X_selfplay.pkl\")\n",
    "    rl_agent_O.save_q_table(\"agent_O_selfplay.pkl\")\n",
    "\n",
    "\n",
    "    # --- Qualitative Check: Play against the trained agent X ---\n",
    "    # Load the agent you want to play against\n",
    "    # rl_agent_X.load_q_table(\"agent_X_vs_random.pkl\") # if trained against random\n",
    "    rl_agent_X.load_q_table(\"agent_X_selfplay.pkl\") # if trained via self-play\n",
    "    # play_against_agent(rl_agent_X, game_env)\n",
    "\n",
    "\n",
    "    # --- Quantitative Evaluation ---\n",
    "    # Example 1: Trained Agent X vs Random Agent O\n",
    "    print(\"\\n--- Evaluating Trained RL Agent (X) vs Random Player (O) ---\")\n",
    "    # rl_agent_X.load_q_table(\"agent_X_selfplay.pkl\") # Ensure it's loaded and epsilon is set low by evaluate_agents\n",
    "    random_eval_opponent = RandomPlayer(player_symbol='O')\n",
    "    evaluate_agents(rl_agent_X, random_eval_opponent, game_env, num_games=1000)\n",
    "\n",
    "    # Example 2: Trained Agent X vs Trained Agent O (from self-play)\n",
    "    print(\"\\n--- Evaluating Trained RL Agent (X) vs Trained RL Agent (O) ---\")\n",
    "    # rl_agent_X.load_q_table(\"agent_X_selfplay.pkl\")\n",
    "    # rl_agent_O.load_q_table(\"agent_O_selfplay.pkl\")\n",
    "    evaluate_agents(rl_agent_X, rl_agent_O, game_env, num_games=1000)\n",
    "\n",
    "    # Example 3: Trained Agent X vs a newly initialized (untrained) RL Agent O\n",
    "    print(\"\\n--- Evaluating Trained RL Agent (X) vs Untrained RL Agent (O) ---\")\n",
    "    untrained_agent_O = RLAgent(player_symbol='O') # Fresh, untrained agent\n",
    "    # rl_agent_X.load_q_table(\"agent_X_selfplay.pkl\")\n",
    "    evaluate_agents(rl_agent_X, untrained_agent_O, game_env, num_games=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7188b06a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting game: Human (O) vs Trained Agent (X)\n",
      "-------------\n",
      "|   |   |   |\n",
      "-------------\n",
      "|   |   |   |\n",
      "-------------\n",
      "|   |   |   |\n",
      "-------------\n",
      "\n",
      "Agent 'X' is thinking...\n",
      "Agent 'X' plays at (1, 1)\n",
      "-------------\n",
      "|   |   |   |\n",
      "-------------\n",
      "|   | X |   |\n",
      "-------------\n",
      "|   |   |   |\n",
      "-------------\n",
      "\n",
      "Available moves: [(0, 0), (0, 1), (0, 2), (1, 0), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "-------------\n",
      "| O |   |   |\n",
      "-------------\n",
      "|   | X |   |\n",
      "-------------\n",
      "|   |   |   |\n",
      "-------------\n",
      "\n",
      "Agent 'X' is thinking...\n",
      "Agent 'X' plays at (0, 1)\n",
      "-------------\n",
      "| O | X |   |\n",
      "-------------\n",
      "|   | X |   |\n",
      "-------------\n",
      "|   |   |   |\n",
      "-------------\n",
      "\n",
      "Available moves: [(0, 2), (1, 0), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "Invalid move. Cell occupied or out of bounds.\n",
      "Available moves: [(0, 2), (1, 0), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "Invalid move. Cell occupied or out of bounds.\n",
      "Available moves: [(0, 2), (1, 0), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
      "-------------\n",
      "| O | X |   |\n",
      "-------------\n",
      "| O | X |   |\n",
      "-------------\n",
      "|   |   |   |\n",
      "-------------\n",
      "\n",
      "Agent 'X' is thinking...\n",
      "Agent 'X' plays at (2, 1)\n",
      "-------------\n",
      "| O | X |   |\n",
      "-------------\n",
      "| O | X |   |\n",
      "-------------\n",
      "|   | X |   |\n",
      "-------------\n",
      "\n",
      "Player 'X' WINS!\n"
     ]
    }
   ],
   "source": [
    "play_against_agent(rl_agent_X, game_env) # Uncomment to play against the trained agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7127d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Interactive Game: Human (O) vs Agent (X) ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0909e1236a54bd9b8666e0af7bf66d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Agent (X) is thinking...'), GridBox(children=(Button(description=' ', disabled=Truâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Circle, Rectangle # Not used in current text drawing, but good for future\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import asyncio # <-- Import asyncio\n",
    "\n",
    "\n",
    "\n",
    "# --- IV. Interactive UI for Playing Against Agent (Corrected) ---\n",
    "class TicTacToeGUI:\n",
    "    def __init__(self, agent, human_player_symbol='O'):\n",
    "        self.env = TicTacToe()\n",
    "        self.agent = agent\n",
    "        self.agent.epsilon = 0 # Agent plays greedily\n",
    "        self.human_player_symbol = human_player_symbol\n",
    "        self.agent_player_symbol = 'X' if human_player_symbol == 'O' else 'O'\n",
    "\n",
    "        # Get the current asyncio event loop\n",
    "        self.loop = asyncio.get_event_loop()\n",
    "\n",
    "        self.buttons = [[widgets.Button(description=' ', layout=widgets.Layout(width='60px', height='60px', margin='2px')) for _ in range(3)] for _ in range(3)]\n",
    "        self.status_label = widgets.Label(value=\"\") # Will be set below\n",
    "        self.reset_button = widgets.Button(description=\"Reset Game\", layout=widgets.Layout(margin='10px 0 0 0'))\n",
    "\n",
    "        self.output_area = widgets.Output()\n",
    "\n",
    "        for r in range(3):\n",
    "            for c in range(3):\n",
    "                self.buttons[r][c].on_click(self.on_button_clicked)\n",
    "                self.buttons[r][c].style.font_weight = 'bold'\n",
    "                self.buttons[r][c].style.font_size = '20px'\n",
    "                self.buttons[r][c].style.button_color = 'lightgray'\n",
    "\n",
    "        self.reset_button.on_click(self.reset_game_clicked) # Renamed to avoid conflict\n",
    "\n",
    "        # Determine initial player and setup board\n",
    "        self.setup_new_game()\n",
    "\n",
    "    def setup_new_game(self):\n",
    "        self.env.reset_board()\n",
    "        self.draw_board_on_output() # Initial draw\n",
    "        for r_idx in range(3):\n",
    "            for c_idx in range(3):\n",
    "                self.buttons[r_idx][c_idx].description = ' '\n",
    "                self.buttons[r_idx][c_idx].disabled = False\n",
    "                self.buttons[r_idx][c_idx].style.button_color = 'lightgray'\n",
    "\n",
    "        if self.agent_player_symbol == 'X': # Agent 'X' starts\n",
    "            self.current_player_symbol = self.agent_player_symbol\n",
    "            self.status_label.value = f\"Agent ({self.agent_player_symbol}) is thinking...\"\n",
    "            self.disable_all_buttons()\n",
    "            self.loop.call_soon(self.agent_turn) # Schedule agent's turn\n",
    "        else: # Human starts (e.g., human is 'X' or agent is 'O')\n",
    "            self.current_player_symbol = self.human_player_symbol\n",
    "            self.status_label.value = f\"Your turn ({self.human_player_symbol}).\"\n",
    "            self.enable_available_buttons()\n",
    "\n",
    "\n",
    "    def draw_board_on_output(self):\n",
    "        with self.output_area:\n",
    "            clear_output(wait=True)\n",
    "            fig, ax = plt.subplots(figsize=(3.5, 3.5)) # Slightly larger figure\n",
    "            ax.set_xlim(0, 3)\n",
    "            ax.set_ylim(0, 3)\n",
    "            # Grid lines\n",
    "            for i in range(4):\n",
    "                ax.plot([i, i], [0, 3], color='black', lw=2) # Vertical\n",
    "                ax.plot([0, 3], [i, i], color='black', lw=2) # Horizontal\n",
    "\n",
    "            ax.tick_params(axis='both', which='both', bottom=False, top=False, left=False, right=False,\n",
    "                           labelbottom=False, labelleft=False)\n",
    "            ax.set_aspect('equal') # Ensure squares are square\n",
    "\n",
    "            for r in range(3):\n",
    "                for c in range(3):\n",
    "                    symbol = self.env.board[r, c]\n",
    "                    if symbol == 'X':\n",
    "                        ax.text(c + 0.5, 2.5 - r, 'X', ha='center', va='center', fontsize=40, color='dodgerblue')\n",
    "                    elif symbol == 'O':\n",
    "                        ax.text(c + 0.5, 2.5 - r, 'O', ha='center', va='center', fontsize=40, color='orangered')\n",
    "            plt.show()\n",
    "\n",
    "    def on_button_clicked(self, b):\n",
    "        if self.env.is_game_over() or self.current_player_symbol == self.agent_player_symbol:\n",
    "            return\n",
    "\n",
    "        move = None\n",
    "        clicked_button_widget = None\n",
    "        for r in range(3):\n",
    "            for c in range(3):\n",
    "                if self.buttons[r][c] == b:\n",
    "                    move = (r, c)\n",
    "                    clicked_button_widget = self.buttons[r][c]\n",
    "                    break\n",
    "            if move: break\n",
    "\n",
    "        if move and self.env.board[move[0], move[1]] == ' ':\n",
    "            self.env.make_move(move, self.human_player_symbol)\n",
    "            clicked_button_widget.description = self.human_player_symbol\n",
    "            clicked_button_widget.disabled = True\n",
    "            clicked_button_widget.style.button_color = 'lightgreen' if self.human_player_symbol == 'O' else 'lightblue'\n",
    "\n",
    "            self.draw_board_on_output()\n",
    "            winner = self.env.check_winner()\n",
    "            if winner:\n",
    "                self.end_game(winner)\n",
    "            else:\n",
    "                self.current_player_symbol = self.agent_player_symbol\n",
    "                self.status_label.value = f\"Agent ({self.agent_player_symbol}) is thinking...\"\n",
    "                self.disable_all_buttons()\n",
    "                self.loop.call_soon(self.agent_turn) # Schedule agent's turn\n",
    "        else:\n",
    "            self.status_label.value = \"Invalid move. Cell occupied or error.\"\n",
    "\n",
    "\n",
    "    def agent_turn(self):\n",
    "        if self.env.is_game_over(): # Check if game ended before agent could move\n",
    "            # This can happen if the human's last move won/drew the game,\n",
    "            # and agent_turn was already scheduled by call_soon.\n",
    "            # The end_game logic would have already run from on_button_clicked.\n",
    "            # Ensure buttons are appropriately disabled if game is over.\n",
    "            if not self.status_label.value.lower().endswith(\"wins!\") and \\\n",
    "               not self.status_label.value.lower().endswith(\"draw!\"):\n",
    "                # If status wasn't updated to a final state by human's win\n",
    "                winner_check = self.env.check_winner()\n",
    "                if winner_check:\n",
    "                    self.end_game(winner_check) # Ensure final state is declared\n",
    "                else: # Should not occur if game_over is true\n",
    "                    self.status_label.value = \"Game Over (Agent Turn Check)\"\n",
    "                    self.disable_all_buttons()\n",
    "            return\n",
    "\n",
    "        board_hash = self.env.get_board_hash()\n",
    "        available_moves = self.env.get_available_moves()\n",
    "        agent_move = self.agent.get_action(board_hash, available_moves)\n",
    "\n",
    "        if agent_move:\n",
    "            self.env.make_move(agent_move, self.agent_player_symbol)\n",
    "            button_to_update = self.buttons[agent_move[0]][agent_move[1]]\n",
    "            button_to_update.description = self.agent_player_symbol\n",
    "            button_to_update.disabled = True\n",
    "            button_to_update.style.button_color = 'lightcoral' if self.agent_player_symbol == 'O' else 'lightskyblue'\n",
    "\n",
    "            self.draw_board_on_output()\n",
    "            winner = self.env.check_winner()\n",
    "            if winner:\n",
    "                self.end_game(winner)\n",
    "            else:\n",
    "                self.current_player_symbol = self.human_player_symbol\n",
    "                self.status_label.value = f\"Your turn ({self.human_player_symbol}).\"\n",
    "                self.enable_available_buttons()\n",
    "        else:\n",
    "            # This case (agent has no move but game not over) should be rare if logic is correct.\n",
    "            # Could happen if available_moves is empty but check_winner didn't declare a draw.\n",
    "            current_winner_status = self.env.check_winner()\n",
    "            if current_winner_status: # Game actually ended\n",
    "                 self.end_game(current_winner_status)\n",
    "            else: # Agent truly stuck\n",
    "                self.status_label.value = \"Agent cannot find a move. Game might be a draw or error.\"\n",
    "                self.end_game('draw') # Declare a draw or investigate\n",
    "\n",
    "\n",
    "    def end_game(self, winner):\n",
    "        self.disable_all_buttons()\n",
    "        if winner == 'draw':\n",
    "            self.status_label.value = \"It's a DRAW!\"\n",
    "        elif winner == self.human_player_symbol:\n",
    "            self.status_label.value = f\"Congratulations! You ({winner}) WIN!\"\n",
    "        elif winner == self.agent_player_symbol:\n",
    "            self.status_label.value = f\"Agent ({winner}) WINS! Better luck next time.\"\n",
    "        else: # Should ideally not be reached if winner is 'X', 'O', or 'draw'\n",
    "             self.status_label.value = \"Game Over.\"\n",
    "\n",
    "\n",
    "    def disable_all_buttons(self):\n",
    "        for r in range(3):\n",
    "            for c in range(3):\n",
    "                self.buttons[r][c].disabled = True\n",
    "\n",
    "    def enable_available_buttons(self):\n",
    "        if self.env.is_game_over(): # Do not enable if game is over\n",
    "            self.disable_all_buttons()\n",
    "            return\n",
    "        for r in range(3):\n",
    "            for c in range(3):\n",
    "                if self.env.board[r,c] == ' ':\n",
    "                    self.buttons[r][c].disabled = False\n",
    "                else:\n",
    "                    self.buttons[r][c].disabled = True\n",
    "\n",
    "\n",
    "    def reset_game_clicked(self, b=None): # Renamed from reset_game\n",
    "        self.setup_new_game()\n",
    "\n",
    "\n",
    "    def display_game(self):\n",
    "        # self.draw_board_on_output() # Initial draw is now in setup_new_game\n",
    "        grid_buttons = []\n",
    "        for r in range(3):\n",
    "            for c_widget in self.buttons[r]:\n",
    "                grid_buttons.append(c_widget)\n",
    "\n",
    "        grid = widgets.GridBox(grid_buttons, layout=widgets.Layout(grid_template_columns=\"repeat(3, 70px)\"))   #(grid_template_columns=\"repeat(3, auto)\"))\n",
    "        display(widgets.VBox([self.status_label, grid, self.output_area, self.reset_button]))\n",
    "\n",
    "\n",
    "# Human will play as 'O', the agent as 'X'\n",
    "print(\"\\n--- Starting Interactive Game: Human (O) vs Agent (X) ---\")\n",
    "# Ensure agent_x_for_play is your trained agent instance\n",
    "interactive_game = TicTacToeGUI(agent=rl_agent_X, human_player_symbol='O')\n",
    "interactive_game.display_game()\n",
    "\n",
    "# To play as 'X' against an agent 'O':\n",
    "# agent_o_for_play = RLAgent(player_symbol='O')\n",
    "# try:\n",
    "#    agent_o_for_play.load_q_table(\"agent_O_selfplay.pkl\") # Assuming you trained an agent 'O'\n",
    "#    print(\"Successfully loaded 'agent_O_selfplay.pkl'.\")\n",
    "# except FileNotFoundError:\n",
    "#    print(\"Could not load 'agent_O_selfplay.pkl'. Agent O will be untrained.\")\n",
    "#\n",
    "# print(\"\\n--- Starting Interactive Game: Human (X) vs Agent (O) ---\")\n",
    "# interactive_game_human_X = TicTacToeGUI(agent=agent_o_for_play, human_player_symbol='X')\n",
    "# interactive_game_human_X.display_game()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d7a7a0",
   "metadata": {},
   "source": [
    "Findings\n",
    "\n",
    "1. Problem: TD(0) with Q-learning fails to learn after 7e5 iterations.\n",
    "Why?:\n",
    "hypothesis 1: One-step TD learning (TD(0)) is very slow because value updates focus on the end of the trajectory. As a result, the beginning of the trajectory receives meaningful value estimates only after a long time.\n",
    "-> It might true. But the true reason of training failure is a critical bug in current version of the code.\n",
    "\n",
    "\n",
    "while not game_over:\n",
    "            available_moves = env.get_available_moves()\n",
    "            if not available_moves: # Should be caught by game_over, but as a safeguard\n",
    "                break\n",
    "\n",
    "            is_agent1_turn = current_player == agent1\n",
    "\n",
    "            if is_agent1_turn:\n",
    "                action = agent1.get_action(current_state_hash, available_moves)\n",
    "                if action:\n",
    "                    # Store state and action for agent1 before making the move\n",
    "                    last_move_info[agent1.player_symbol] = {'state': current_state_hash, 'action': action}\n",
    "                    env.make_move(action, agent1.player_symbol)\n",
    "                else: # No valid action (should not happen with available_moves check)\n",
    "                    game_over = True # End game if agent can't move\n",
    "                    break\n",
    "            else: # Opponent's turn (could be another RLAgent or a random one)\n",
    "                if isinstance(agent2_opponent, RLAgent):\n",
    "                    action = agent2_opponent.get_action(current_state_hash, available_moves)\n",
    "                    if action:\n",
    "                        # Store state and action for agent2 if it's an RL agent\n",
    "                        last_move_info[agent2_opponent.player_symbol] = {'state': current_state_hash, 'action': action}\n",
    "                        env.make_move(action, agent2_opponent.player_symbol)\n",
    "                    else:\n",
    "                        game_over = True\n",
    "                        break\n",
    "                else: # Random opponent\n",
    "                    action = random.choice(available_moves)\n",
    "                    env.make_move(action, agent2_opponent.player_symbol) # Opponent symbol\n",
    "\n",
    "            winner = env.check_winner()\n",
    "            game_over = (winner is not None)\n",
    "            next_state_hash = env.get_board_hash()\n",
    "            next_available_moves = env.get_available_moves()\n",
    "\n",
    "            # --- THIS IS WHERE THE TRAINING ALGORITHM IS APPLIED ---\n",
    "            ### But what if the game is over? We need to update the Q-table for the last move but it's not what done in here\n",
    "            # Update Q-table for agent1 based on its last move\n",
    "            if last_move_info[agent1.player_symbol]:\n",
    "                s, a = last_move_info[agent1.player_symbol]['state'], last_move_info[agent1.player_symbol]['action']\n",
    "                reward_agent1 = env.get_reward(agent1.player_symbol, winner)\n",
    "                agent1.update_q_table(s, a, reward_agent1, next_state_hash, next_available_moves, game_over)\n",
    "                last_move_info[agent1.player_symbol] = None # Clear after update\n",
    "\n",
    "            # Update Q-table for agent2 if it's an RLAgent and made a move\n",
    "            if isinstance(agent2_opponent, RLAgent) and last_move_info[agent2_opponent.player_symbol]:\n",
    "                s, a = last_move_info[agent2_opponent.player_symbol]['state'], last_move_info[agent2_opponent.player_symbol]['action']\n",
    "                reward_agent2 = env.get_reward(agent2_opponent.player_symbol, winner)\n",
    "                # Note: The reward for agent2 is based on the same game outcome\n",
    "                agent2_opponent.update_q_table(s, a, reward_agent2, next_state_hash, next_available_moves, game_over)\n",
    "                last_move_info[agent2_opponent.player_symbol] = None\n",
    "\n",
    "\n",
    "        => In here, when it is agent_1_turn the next_state_hash is the state after reflecting agent_1's action. Do you see what is the problem? The next state, which is given to agent 1 and used to select the max value action that agent 1 can take, should be the state after agent_2's action is taken. So in Q-learning, the true target value is always 0 unless it is the very before the termination state(where reward is not zero). Because it tries to use the estimation of non-existing state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db9848e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4dfc8f11",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
